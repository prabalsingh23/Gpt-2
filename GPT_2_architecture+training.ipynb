{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uk_4FYc8NMWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "vNRe16TIM0WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "LMjYasC7M4F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size= 1024\n",
        "vocab_size= 50257 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "\n",
        "n_layer= 12\n",
        "n_pos=1024\n",
        "n_head= 12\n",
        "n_embd= 768\n",
        "dropout = 0.1\n",
        "bias= True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ],
      "metadata": {
        "id": "mhxOGDKBMwUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def _init_(self, d_model, max_len=512):\n",
        "        super(PositionalEncoding, self)._init_()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].detach()\n",
        "\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "\n",
        "    def __init__(self, nf, nx):\n",
        "        super().__init__()\n",
        "        self.nf = nf\n",
        "        self.weight = nn.Parameter(torch.empty(nx, nf))\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "        nn.init.normal_(self.weight, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(size_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NewGELUActivation(nn.Module):\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
        "\n"
      ],
      "metadata": {
        "id": "lJkjEzfENNs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Attention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = Conv1D(3*n_embd,n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = Conv1D(n_embd, n_embd)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout =dropout\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class GPT2MLP(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.c_fc    = Conv1D(4*n_embd,  n_embd)\n",
        "        self.c_proj  = Conv1D(n_embd, 4*n_embd)\n",
        "        self.act    = NewGELUActivation()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.act(x)\n",
        "        x = self.c_proj(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "OrNgq-xcNU__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Block(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True)\n",
        "        self.attn = GPT2Attention()\n",
        "        self.ln_2 = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True)\n",
        "        self.mlp = GPT2MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(vocab_size, n_embd),\n",
        "            wpe = nn.Embedding(block_size, n_embd),\n",
        "            drop = nn.Dropout(dropout),\n",
        "            h = nn.ModuleList([GPT2Block() for _ in range(n_layer)]),\n",
        "            ln_f = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= block_size, f\"Cannot forward sequence of length {t}, block size is only {block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for GPT2Block in self.transformer.h:\n",
        "            x = GPT2Block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "E2u2ChzqNYQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the state dictionary of your GPT2Small model\n",
        "model = GPT()\n",
        "# model=model.to(device)\n",
        "model_state_dict = model.state_dict()\n"
      ],
      "metadata": {
        "id": "TCf_gLBCNczK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "pretrained_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "pretrained_state_dict = pretrained_model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB0erJFlNeaV",
        "outputId": "362b89d1-7f02-4b4f-b26b-bb00c96fc25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fb988325a1144f8a89ee6176302e8f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "042614e6376e4aa2a912feda7e3bc85a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7103f706f47e4b6490215cc4dae2887b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in pretrained_state_dict.items():\n",
        "    # print(name)\n",
        "    if name in model_state_dict and param.size() == model_state_dict[name].size():\n",
        "        model_state_dict[name].copy_(param)\n",
        "\n"
      ],
      "metadata": {
        "id": "45WBPxw5NhK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,param in model_state_dict.items():\n",
        "  print(name,torch.equal(model_state_dict[name],pretrained_state_dict[name]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB0Q6POQNkic",
        "outputId": "c11ee88b-a887-4d59-e652-f3b0c906a076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight True\n",
            "transformer.wpe.weight True\n",
            "transformer.h.0.ln_1.weight True\n",
            "transformer.h.0.ln_1.bias True\n",
            "transformer.h.0.attn.c_attn.weight True\n",
            "transformer.h.0.attn.c_attn.bias True\n",
            "transformer.h.0.attn.c_proj.weight True\n",
            "transformer.h.0.attn.c_proj.bias True\n",
            "transformer.h.0.ln_2.weight True\n",
            "transformer.h.0.ln_2.bias True\n",
            "transformer.h.0.mlp.c_fc.weight True\n",
            "transformer.h.0.mlp.c_fc.bias True\n",
            "transformer.h.0.mlp.c_proj.weight True\n",
            "transformer.h.0.mlp.c_proj.bias True\n",
            "transformer.h.1.ln_1.weight True\n",
            "transformer.h.1.ln_1.bias True\n",
            "transformer.h.1.attn.c_attn.weight True\n",
            "transformer.h.1.attn.c_attn.bias True\n",
            "transformer.h.1.attn.c_proj.weight True\n",
            "transformer.h.1.attn.c_proj.bias True\n",
            "transformer.h.1.ln_2.weight True\n",
            "transformer.h.1.ln_2.bias True\n",
            "transformer.h.1.mlp.c_fc.weight True\n",
            "transformer.h.1.mlp.c_fc.bias True\n",
            "transformer.h.1.mlp.c_proj.weight True\n",
            "transformer.h.1.mlp.c_proj.bias True\n",
            "transformer.h.2.ln_1.weight True\n",
            "transformer.h.2.ln_1.bias True\n",
            "transformer.h.2.attn.c_attn.weight True\n",
            "transformer.h.2.attn.c_attn.bias True\n",
            "transformer.h.2.attn.c_proj.weight True\n",
            "transformer.h.2.attn.c_proj.bias True\n",
            "transformer.h.2.ln_2.weight True\n",
            "transformer.h.2.ln_2.bias True\n",
            "transformer.h.2.mlp.c_fc.weight True\n",
            "transformer.h.2.mlp.c_fc.bias True\n",
            "transformer.h.2.mlp.c_proj.weight True\n",
            "transformer.h.2.mlp.c_proj.bias True\n",
            "transformer.h.3.ln_1.weight True\n",
            "transformer.h.3.ln_1.bias True\n",
            "transformer.h.3.attn.c_attn.weight True\n",
            "transformer.h.3.attn.c_attn.bias True\n",
            "transformer.h.3.attn.c_proj.weight True\n",
            "transformer.h.3.attn.c_proj.bias True\n",
            "transformer.h.3.ln_2.weight True\n",
            "transformer.h.3.ln_2.bias True\n",
            "transformer.h.3.mlp.c_fc.weight True\n",
            "transformer.h.3.mlp.c_fc.bias True\n",
            "transformer.h.3.mlp.c_proj.weight True\n",
            "transformer.h.3.mlp.c_proj.bias True\n",
            "transformer.h.4.ln_1.weight True\n",
            "transformer.h.4.ln_1.bias True\n",
            "transformer.h.4.attn.c_attn.weight True\n",
            "transformer.h.4.attn.c_attn.bias True\n",
            "transformer.h.4.attn.c_proj.weight True\n",
            "transformer.h.4.attn.c_proj.bias True\n",
            "transformer.h.4.ln_2.weight True\n",
            "transformer.h.4.ln_2.bias True\n",
            "transformer.h.4.mlp.c_fc.weight True\n",
            "transformer.h.4.mlp.c_fc.bias True\n",
            "transformer.h.4.mlp.c_proj.weight True\n",
            "transformer.h.4.mlp.c_proj.bias True\n",
            "transformer.h.5.ln_1.weight True\n",
            "transformer.h.5.ln_1.bias True\n",
            "transformer.h.5.attn.c_attn.weight True\n",
            "transformer.h.5.attn.c_attn.bias True\n",
            "transformer.h.5.attn.c_proj.weight True\n",
            "transformer.h.5.attn.c_proj.bias True\n",
            "transformer.h.5.ln_2.weight True\n",
            "transformer.h.5.ln_2.bias True\n",
            "transformer.h.5.mlp.c_fc.weight True\n",
            "transformer.h.5.mlp.c_fc.bias True\n",
            "transformer.h.5.mlp.c_proj.weight True\n",
            "transformer.h.5.mlp.c_proj.bias True\n",
            "transformer.h.6.ln_1.weight True\n",
            "transformer.h.6.ln_1.bias True\n",
            "transformer.h.6.attn.c_attn.weight True\n",
            "transformer.h.6.attn.c_attn.bias True\n",
            "transformer.h.6.attn.c_proj.weight True\n",
            "transformer.h.6.attn.c_proj.bias True\n",
            "transformer.h.6.ln_2.weight True\n",
            "transformer.h.6.ln_2.bias True\n",
            "transformer.h.6.mlp.c_fc.weight True\n",
            "transformer.h.6.mlp.c_fc.bias True\n",
            "transformer.h.6.mlp.c_proj.weight True\n",
            "transformer.h.6.mlp.c_proj.bias True\n",
            "transformer.h.7.ln_1.weight True\n",
            "transformer.h.7.ln_1.bias True\n",
            "transformer.h.7.attn.c_attn.weight True\n",
            "transformer.h.7.attn.c_attn.bias True\n",
            "transformer.h.7.attn.c_proj.weight True\n",
            "transformer.h.7.attn.c_proj.bias True\n",
            "transformer.h.7.ln_2.weight True\n",
            "transformer.h.7.ln_2.bias True\n",
            "transformer.h.7.mlp.c_fc.weight True\n",
            "transformer.h.7.mlp.c_fc.bias True\n",
            "transformer.h.7.mlp.c_proj.weight True\n",
            "transformer.h.7.mlp.c_proj.bias True\n",
            "transformer.h.8.ln_1.weight True\n",
            "transformer.h.8.ln_1.bias True\n",
            "transformer.h.8.attn.c_attn.weight True\n",
            "transformer.h.8.attn.c_attn.bias True\n",
            "transformer.h.8.attn.c_proj.weight True\n",
            "transformer.h.8.attn.c_proj.bias True\n",
            "transformer.h.8.ln_2.weight True\n",
            "transformer.h.8.ln_2.bias True\n",
            "transformer.h.8.mlp.c_fc.weight True\n",
            "transformer.h.8.mlp.c_fc.bias True\n",
            "transformer.h.8.mlp.c_proj.weight True\n",
            "transformer.h.8.mlp.c_proj.bias True\n",
            "transformer.h.9.ln_1.weight True\n",
            "transformer.h.9.ln_1.bias True\n",
            "transformer.h.9.attn.c_attn.weight True\n",
            "transformer.h.9.attn.c_attn.bias True\n",
            "transformer.h.9.attn.c_proj.weight True\n",
            "transformer.h.9.attn.c_proj.bias True\n",
            "transformer.h.9.ln_2.weight True\n",
            "transformer.h.9.ln_2.bias True\n",
            "transformer.h.9.mlp.c_fc.weight True\n",
            "transformer.h.9.mlp.c_fc.bias True\n",
            "transformer.h.9.mlp.c_proj.weight True\n",
            "transformer.h.9.mlp.c_proj.bias True\n",
            "transformer.h.10.ln_1.weight True\n",
            "transformer.h.10.ln_1.bias True\n",
            "transformer.h.10.attn.c_attn.weight True\n",
            "transformer.h.10.attn.c_attn.bias True\n",
            "transformer.h.10.attn.c_proj.weight True\n",
            "transformer.h.10.attn.c_proj.bias True\n",
            "transformer.h.10.ln_2.weight True\n",
            "transformer.h.10.ln_2.bias True\n",
            "transformer.h.10.mlp.c_fc.weight True\n",
            "transformer.h.10.mlp.c_fc.bias True\n",
            "transformer.h.10.mlp.c_proj.weight True\n",
            "transformer.h.10.mlp.c_proj.bias True\n",
            "transformer.h.11.ln_1.weight True\n",
            "transformer.h.11.ln_1.bias True\n",
            "transformer.h.11.attn.c_attn.weight True\n",
            "transformer.h.11.attn.c_attn.bias True\n",
            "transformer.h.11.attn.c_proj.weight True\n",
            "transformer.h.11.attn.c_proj.bias True\n",
            "transformer.h.11.ln_2.weight True\n",
            "transformer.h.11.ln_2.bias True\n",
            "transformer.h.11.mlp.c_fc.weight True\n",
            "transformer.h.11.mlp.c_fc.bias True\n",
            "transformer.h.11.mlp.c_proj.weight True\n",
            "transformer.h.11.mlp.c_proj.bias True\n",
            "transformer.ln_f.weight True\n",
            "transformer.ln_f.bias True\n",
            "lm_head.weight True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'  # or 'gpt2-medium', 'gpt2-large', depending on the size you want\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(\"hii\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3cWHu9qNo1-",
        "outputId": "1140118a-90b7-4ac7-9541-6f05db4a81cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31c010b39a5e4748bd937724a40576a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49a8f286d12548d1851e631d2dce809f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59b2fe83f5904173be2516b1c0249b89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hii\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLuSL79ONx6V",
        "outputId": "de3a0a91-a67d-4fdc-a7d5-a9c66754d8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "# Sample input text\n",
        "input_text = \"Once upon a time\"\n",
        "\n",
        "model=model.to(device)\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "# Generate predictions\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_ids, max_new_tokens=50)\n",
        "\n",
        "# Decode the generated output\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated Text:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nGsF9S8N0-B",
        "outputId": "80876e87-d88f-45f5-a6c5-cbddb7e55ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: Once upon a time, any consigned to copious medical terms would have so far been accursed and refused medical practices.\n",
            "\n",
            "But no less emphatic under the legal code, physicians who had previously been found guilty were punished. They were closed prisons, and life\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "pretrained_model=pretrained_model.to(device)\n",
        "pretrained_model.eval()\n",
        "with torch.no_grad():\n",
        "    output_ = pretrained_model.generate(input_ids, max_new_tokens=50)\n",
        "\n",
        "generated_text_=tokenizer.decode(output_[0], skip_special_tokens=True)\n",
        "print(\"Generated Text by GPT:\", generated_text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nKCSMcwN28K",
        "outputId": "7b3c0dc8-53da-43ef-83f8-7b3f5ab50a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text by GPT: Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Shakespere Data"
      ],
      "metadata": {
        "id": "TxoI8dqZRRx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('/content/input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "46KFFJIPJTTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_FljB8HJXYz",
        "outputId": "73aabab6-8465-4116-cda1-8506da693d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFkd3q8IJZcJ",
        "outputId": "04142f34-cbb2-44e6-c254-2e003ba0c254"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mlORIaKJbCq",
        "outputId": "dd21e8cd-7c62-4b52-ceaf-51b168f3a540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0PzdYP8EOfh",
        "outputId": "2f43a6b7-e115-4a86-aa73-5c4501b6ff9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "GQrNDitVYns7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_encoded=torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data_encoded.shape, data_encoded.dtype)\n",
        "print(data_encoded[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "id": "ynBO8M0BEQtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data_encoded)) # first 90% will be train, rest val\n",
        "train_data_encoded = data_encoded[:n]\n",
        "val_data_encoded = data_encoded[n:]"
      ],
      "metadata": {
        "id": "40Pj2D8ZEd7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 50\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "nAaXAKFFGJI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "# batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "# block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data_encoded if split == 'train' else val_data_encoded\n",
        "    ix = torch.randint(len(data_encoded) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_encoded[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_encoded[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tmhGO9hEoNZ",
        "outputId": "4912a035-247b-4b84-c6f9-c551abb86c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([64, 256])\n",
            "tensor([[59, 58,  1,  ..., 39, 57,  6],\n",
            "        [43, 56,  1,  ..., 37, 43, 57],\n",
            "        [58, 46, 53,  ..., 17, 26, 14],\n",
            "        ...,\n",
            "        [46, 43, 39,  ..., 42, 43, 57],\n",
            "        [18, 18, 10,  ..., 53, 56,  1],\n",
            "        [45, 46, 39,  ...,  1, 58, 46]])\n",
            "targets:\n",
            "torch.Size([64, 256])\n",
            "tensor([[58,  1, 15,  ..., 57,  6,  1],\n",
            "        [56,  1, 24,  ..., 43, 57,  6],\n",
            "        [46, 53, 59,  ..., 26, 14, 33],\n",
            "        ...,\n",
            "        [43, 39, 56,  ..., 43, 57, 54],\n",
            "        [18, 10,  0,  ..., 56,  1, 40],\n",
            "        [46, 39, 51,  ..., 58, 46, 53]])\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model.."
      ],
      "metadata": {
        "id": "xLGeRvScO05J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "Vreepv4nPFyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_size=65\n",
        "\n",
        "n_pos=1024\n",
        "\n",
        "bias= True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ],
      "metadata": {
        "id": "ZAJ1oXbxPOCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "weE6lbv2PO7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training our base model on dataset.."
      ],
      "metadata": {
        "id": "Oio2ixqNT7YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "model=GPT()\n",
        "model=model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "81I4Zw8RT-ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XhIE5VSVSjr",
        "outputId": "faec522e-3248-4a52-cdc3-d5ec94622dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(65, 384)\n",
              "    (wpe): Embedding(256, 384)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 100\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X,Y=X.to(device),Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "ld8duxBJUMrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU94x7JlVCXc",
        "outputId": "7a45a4e1-8a9a-4aaa-bbd6-b2647e3b632a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "?qfzxbDkRZkNdc'wj,ZT,OLFL,eHtK\n",
            "bHiPjCkMBbeAu3:XaSvgO-3qjMBF?gLUauhX:LVULFQ&eNuwqcOMxv.t?Vr dXlrDZaoeNFw3XHPpvWk,fDE$nYzxzqjNmX\n",
            "Yo3&$FMtofViEIiB!!&VmOW;Kd!lKx,Ke3 ixYeYERnXciK;lxW;HFGidroG EsSXUB;qWk p.YGD3.lYWjbm!pelJlLnFAmVQF.C-hx;3qcncwvbN:?Uuv;MaiT'X3Uwty;MJlvBPUHI.yBm&pjY-lgvIEjMk:DGyqwJdqGMtSkklmoyW-SQA&QhdGC\n",
            "Iib3qM'yS!-&fM$HZLETxgGGhx&$FsgC-LB3:Ae-xT3H\n",
            "hAxkMMmnvbrufWqA s\n",
            ";;3;QDLWTZ:fvt,Cdy.vlMUE$,w,fMFMPRD?CqYLSoB.UrHK-NLbk!ar,$yb&i&:\n",
            ":rdsabWG$!JEgDLHYBvuihJKNuk?Dyr?:nyHRrxutM-I&fy&VE?!NMJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    xb,yb=xb.to(device),yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4ms2q00UR6r",
        "outputId": "ec72882e-cb3c-493f-efd2-c6a83a9aa73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3321, val loss 4.3311\n",
            "step 50: train loss 2.4802, val loss 2.4817\n",
            "step 100: train loss 2.4320, val loss 2.4320\n",
            "step 150: train loss 2.3295, val loss 2.3314\n",
            "step 200: train loss 2.1803, val loss 2.1808\n",
            "step 250: train loss 2.0407, val loss 2.0401\n",
            "step 300: train loss 1.9079, val loss 1.9108\n",
            "step 350: train loss 1.8039, val loss 1.8058\n",
            "step 400: train loss 1.7218, val loss 1.7216\n",
            "step 450: train loss 1.6575, val loss 1.6572\n",
            "step 500: train loss 1.6021, val loss 1.6028\n",
            "step 550: train loss 1.5627, val loss 1.5632\n",
            "step 600: train loss 1.5198, val loss 1.5246\n",
            "step 650: train loss 1.4880, val loss 1.4887\n",
            "step 700: train loss 1.4615, val loss 1.4634\n",
            "step 750: train loss 1.4320, val loss 1.4287\n",
            "step 800: train loss 1.4096, val loss 1.4108\n",
            "step 850: train loss 1.3866, val loss 1.3932\n",
            "step 900: train loss 1.3692, val loss 1.3704\n",
            "step 950: train loss 1.3527, val loss 1.3531\n",
            "step 999: train loss 1.3373, val loss 1.3403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkF3LES0bj_5",
        "outputId": "e59ef9f7-be84-4acb-f8c2-6907ed4e8cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Commen he knock!\n",
            "\n",
            "LEONTES:\n",
            "Sir, thus prithee?\n",
            "\n",
            "FER:\n",
            "Yet you fieur bed; for safter you.\n",
            "\n",
            "PETRUCHIO:\n",
            "A darest memchanns. Thou love discoce;\n",
            "Indoled live make good so bise;\n",
            "This what I have-rogs no; become yet bear mocked\n",
            "And 'twas spick'd; and I am again.\n",
            "\n",
            "GRUMIO:\n",
            "The con I stavest\n",
            "Shorter the forbired of your lajest inderath!\n",
            "\n",
            "GRUMIO:\n",
            "Now nay; brave! Ka\n",
            "hang been, the nurse I will before\n",
            "What wicke, I am them have bold and I head,\n",
            "And what yet to grant carge years;\n",
            "And thou thee were you child, y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotary Position Encoding"
      ],
      "metadata": {
        "id": "99Sh92tjRsZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=n_pos):\n",
        "        super().__init__()\n",
        "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float)\n",
        "        sinusoid_inp = torch.einsum(\"i,j->ij\", position, inv_freq)\n",
        "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
        "        self.register_buffer('emb', emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb[None,:x.shape[1], :].to(x)"
      ],
      "metadata": {
        "id": "RItT_CEOe7cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_NpGwajf4fS",
        "outputId": "d7f51082-088a-4361-bcfb-ef0548f8ee9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSI_X3.4-1968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "Cjb7-FC3gCp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sHEQdwYfsdh",
        "outputId": "2558bdfb-d670-4d3c-c17b-6a1b6afc7410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, repeat\n"
      ],
      "metadata": {
        "id": "C-2ow_KIfqdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_every_two(x):\n",
        "    x = rearrange(x, '... (d j) -> ... d j', j = 2)\n",
        "    x1, x2 = x.unbind(dim = -1)\n",
        "    x = torch.stack((-x2, x1), dim = -1)\n",
        "    return rearrange(x, '... d j -> ... (d j)')\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, sinu_pos):\n",
        "    sinu_pos = rearrange(sinu_pos, '() n (j d) -> n j d', j = 2)\n",
        "    sin, cos = sinu_pos.unbind(dim = -2)\n",
        "    sin, cos = map(lambda t: repeat(t, 'b n -> b (n j)', j = 2), (sin, cos))\n",
        "    q, k = map(lambda t: (t * cos) + (rotate_every_two(t) * sin), (q, k))\n",
        "    return q, k"
      ],
      "metadata": {
        "id": "YcalqpQZfUTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Attention_rotary(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = Conv1D(3*n_embd,n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = Conv1D(n_embd, n_embd)\n",
        "\n",
        "        self.pos=FixedPositionalEmbedding(n_embd// n_head)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout =dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        x=self.c_attn(x)\n",
        "\n",
        "        pos=self.pos(x)\n",
        "        q, k, v  = x.split(self.n_embd, dim=2)\n",
        "        # print(q.shape,k.shape,v.shape)\n",
        "\n",
        "        # Implementing rotary embedding..\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        q, k = apply_rotary_pos_emb(q, k, pos)\n",
        "        # print(q.shape,k.shape)\n",
        "\n",
        "\n",
        "\n",
        "        # manual implementation of attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vnbr-_uFP1FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Block_rotary(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True)\n",
        "        self.attn = GPT2Attention_rotary()\n",
        "        self.ln_2 = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True)\n",
        "        self.mlp = GPT2MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT_rotary(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(vocab_size, n_embd),\n",
        "            wpe = nn.Embedding(block_size, n_embd),\n",
        "            drop = nn.Dropout(dropout),\n",
        "            h = nn.ModuleList([GPT2Block_rotary() for _ in range(n_layer)]),\n",
        "            ln_f = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= block_size, f\"Cannot forward sequence of length {t}, block size is only {block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for GPT2Block_rotary in self.transformer.h:\n",
        "            x = GPT2Block_rotary(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "zaocuUnzTq1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now training our rotary embedding based model on this dataset..."
      ],
      "metadata": {
        "id": "G4tXg6cbQFCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rotary=GPT_rotary()\n",
        "model_rotary=model_rotary.to(device)"
      ],
      "metadata": {
        "id": "v5aLrx21WO1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model_rotary.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "74jt9ekNQObn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 100\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X,Y=X.to(device),Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "Kdiaz3m3RwLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(model_rotary)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    xb,yb=xb.to(device),yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model_rotary(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmCLpLPBIAxk",
        "outputId": "6e64391c-e47d-412d-b8f1-9e5c2e7f1223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3535, val loss 4.3526\n",
            "step 50: train loss 2.2412, val loss 2.2424\n",
            "step 100: train loss 1.9550, val loss 1.9551\n",
            "step 150: train loss 1.8149, val loss 1.8121\n",
            "step 200: train loss 1.7073, val loss 1.7086\n",
            "step 250: train loss 1.6343, val loss 1.6342\n",
            "step 300: train loss 1.5841, val loss 1.5778\n",
            "step 350: train loss 1.5349, val loss 1.5334\n",
            "step 400: train loss 1.4982, val loss 1.4994\n",
            "step 450: train loss 1.4665, val loss 1.4692\n",
            "step 500: train loss 1.4434, val loss 1.4442\n",
            "step 550: train loss 1.4155, val loss 1.4171\n",
            "step 600: train loss 1.3955, val loss 1.3960\n",
            "step 650: train loss 1.3791, val loss 1.3761\n",
            "step 700: train loss 1.3580, val loss 1.3582\n",
            "step 750: train loss 1.3473, val loss 1.3461\n",
            "step 800: train loss 1.3250, val loss 1.3281\n",
            "step 850: train loss 1.3212, val loss 1.3170\n",
            "step 900: train loss 1.3052, val loss 1.3031\n",
            "step 950: train loss 1.2961, val loss 1.2944\n",
            "step 999: train loss 1.2831, val loss 1.2822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model_rotary.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taKUgGS5JBhj",
        "outputId": "a75b3855-38ef-4b75-b894-88af460081d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "shall be not mon vex'da, mark thy braile womph'd.\n",
            "\n",
            "First Much Dick,\n",
            "Met your compass to warm; and whilest here.\n",
            "\n",
            "VOLUMNIA:\n",
            "The wamons here, done about a hour, let night meat;\n",
            "For of Warwick splent, any one-semplay.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Thereto my sure well with their eye of all?\n",
            "\n",
            "CAXLILF:\n",
            "Nay! away!\n",
            "\n",
            "TYRREL:\n",
            "God love for Willanus fortune's peach'd.\n",
            "\n",
            "RICHARD:\n",
            "Ay, I sirraw hate?\n",
            "\n",
            "ARCHISS SEBY:\n",
            "Ay, this not would dangers despecting and teas:\n",
            "Tell then I seed?\n",
            "\n",
            "POMPEY:\n",
            "O, Petruchurs, singulabol!--\n",
            "Under\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Group Query Attention.."
      ],
      "metadata": {
        "id": "ok3JEnIJX8RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Attention_query(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "\n",
        "        self.num_groups = 4\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = Conv1D(3*n_embd,n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = Conv1D(n_embd, n_embd)\n",
        "\n",
        "        self.pos=FixedPositionalEmbedding(n_embd// n_head)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout =dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        x=self.c_attn(x)\n",
        "\n",
        "        pos=self.pos(x)\n",
        "        q, k, v  = x.split(self.n_embd, dim=2)\n",
        "        # print(q.shape,k.shape,v.shape)\n",
        "\n",
        "        # Implementing rotary embedding..\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        q, k = apply_rotary_pos_emb(q, k, pos)\n",
        "        # print(q.shape,k.shape)\n",
        "\n",
        "        # Apply group query attention..\n",
        "        k_new=k.clone()\n",
        "        q_new=q.clone()\n",
        "        v_new=v.clone()\n",
        "        # print(k_new.shape)\n",
        "        for i in range(0,n_head//self.num_groups):\n",
        "          k_new[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:]=torch.mean(k[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:])\n",
        "          q_new[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:]=torch.mean(q[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:])\n",
        "          v_new[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:]=torch.mean(v[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:])\n",
        "\n",
        "        k=k_new\n",
        "        q=q_new\n",
        "        v=v_new\n",
        "\n",
        "\n",
        "        # manual implementation of attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n"
      ],
      "metadata": {
        "id": "8VKaq3boYBqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Block_query(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True)\n",
        "        self.attn = GPT2Attention_query()\n",
        "        self.ln_2 = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True)\n",
        "        self.mlp = GPT2MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT_query(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(vocab_size, n_embd),\n",
        "            wpe = nn.Embedding(block_size, n_embd),\n",
        "            drop = nn.Dropout(dropout),\n",
        "            h = nn.ModuleList([GPT2Block_query() for _ in range(n_layer)]),\n",
        "            ln_f = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True),\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= block_size, f\"Cannot forward sequence of length {t}, block size is only {block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for GPT2Block_query in self.transformer.h:\n",
        "            x = GPT2Block_query(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "PLcO18INY98p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_query=GPT_query()\n",
        "model_query_state_dict=model_query.state_dict()\n",
        "model_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsdBmAkxZCLF",
        "outputId": "3b4c176a-5e00-4e30-cecf-1b76fece1eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT_query(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(65, 384)\n",
              "    (wpe): Embedding(256, 384)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block_query(\n",
              "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention_modified(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (pos): FixedPositionalEmbedding()\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model_query.parameters())/1e6, 'M parameters')\n",
        "#   print(name,torch.equal(model_query_state_dict[name],pretrained_state_dict[name]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbm0rXXYZ51Y",
        "outputId": "072b2031-90e9-42cf-c61d-dfab00952237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.795776 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model_query.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "t6dPQJzma38Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 100\n",
        "model_query=model_query.to(device)\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X,Y=X.to(device),Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "t8nFQ-HQ8Zf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(model_query)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    xb,yb=xb.to(device),yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model_query(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzEoXacA1zGV",
        "outputId": "d547aa3c-1ee4-42e0-e67f-e742c55c220d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3438, val loss 4.3442\n",
            "step 50: train loss 2.2474, val loss 2.2465\n",
            "step 100: train loss 1.9610, val loss 1.9589\n",
            "step 150: train loss 1.8163, val loss 1.8139\n",
            "step 200: train loss 1.7083, val loss 1.7071\n",
            "step 250: train loss 1.6337, val loss 1.6366\n",
            "step 300: train loss 1.5782, val loss 1.5792\n",
            "step 350: train loss 1.5342, val loss 1.5361\n",
            "step 400: train loss 1.4958, val loss 1.5023\n",
            "step 450: train loss 1.4631, val loss 1.4607\n",
            "step 500: train loss 1.4432, val loss 1.4451\n",
            "step 550: train loss 1.4122, val loss 1.4146\n",
            "step 600: train loss 1.3908, val loss 1.3897\n",
            "step 650: train loss 1.3752, val loss 1.3749\n",
            "step 700: train loss 1.3637, val loss 1.3659\n",
            "step 750: train loss 1.3414, val loss 1.3400\n",
            "step 800: train loss 1.3257, val loss 1.3268\n",
            "step 850: train loss 1.3150, val loss 1.3125\n",
            "step 900: train loss 1.3021, val loss 1.3019\n",
            "step 950: train loss 1.2923, val loss 1.2891\n",
            "step 999: train loss 1.2785, val loss 1.2804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model_query.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z41JMypQoGYP",
        "outputId": "e3d508a8-95a5-40dd-90a7-b695844a8360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The prite alift marriage her dischret,\n",
            "May brave withbrand, heart can, man for them,\n",
            "And before her authority away'd hear;\n",
            "Or whom expromong with thy shumble speaks venesion.\n",
            "\n",
            "Lord:\n",
            "A is the passsions sat may had lift bid.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "O mustand the world, sir, to turned in execute's\n",
            "muster armend it.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Lathermen, haste inter life of firer looks.\n",
            "Is you a good suspect your life, and as a noble at\n",
            "well to Jove as an ear. How better upon and him,\n",
            "it hath brang'd him crass whi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sliding Window Attention"
      ],
      "metadata": {
        "id": "Z-bw-DbQ8vJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SlidingWindowAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, window_size=64):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        # Implement sliding window attention\n",
        "        B, nh, T, hs = q.size()\n",
        "\n",
        "        # Calculate the windowed attention\n",
        "        attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        mask1 = torch.tril(torch.ones(1, T, T), diagonal=self.window_size)\n",
        "        mask2= torch.triu(torch.ones(1, T, T), diagonal= -self.window_size)\n",
        "\n",
        "        mask=torch.einsum('ijk,ijk -> ijk',mask1,mask2)\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = attn.masked_fill(torch.isnan(attn), 0.0)\n",
        "\n",
        "        # Apply the attention to the values\n",
        "        x = attn @ v\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "tcpIChLz-aZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Attention_sliding(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "\n",
        "        self.num_groups = 4\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = Conv1D(3*n_embd,n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = Conv1D(n_embd, n_embd)\n",
        "\n",
        "        self.pos=FixedPositionalEmbedding(n_embd// n_head)\n",
        "        # regularization\n",
        "\n",
        "        self.sliding_window_attention=SlidingWindowAttention()\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout =dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        x=self.c_attn(x)\n",
        "\n",
        "        pos=self.pos(x)\n",
        "        q, k, v  = x.split(self.n_embd, dim=2)\n",
        "        # print(q.shape,k.shape,v.shape)\n",
        "\n",
        "        # Implementing rotary embedding..\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        q, k = apply_rotary_pos_emb(q, k, pos)\n",
        "        # print(q.shape,k.shape)\n",
        "\n",
        "        # Apply group query attention..\n",
        "        k_new=k.clone()\n",
        "        q_new=q.clone()\n",
        "        v_new=v.clone()\n",
        "        # print(k_new.shape)\n",
        "        for i in range(0,n_head//self.num_groups):\n",
        "          k_new[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:]=torch.mean(k[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:])\n",
        "          q_new[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:]=torch.mean(q[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:])\n",
        "          v_new[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:]=torch.mean(v[:,(i)*(n_head//self.num_groups):(i+1)*(n_head//self.num_groups)-1,:,:])\n",
        "\n",
        "        k=k_new\n",
        "        q=q_new\n",
        "        v=v_new\n",
        "\n",
        "\n",
        "        y = self.sliding_window_attention(q, k, v)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "uhgoNCICAFRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Block_sliding(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True)\n",
        "        self.attn = GPT2Attention_sliding()\n",
        "        self.ln_2 = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True)\n",
        "        self.mlp = GPT2MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT_sliding(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(vocab_size, n_embd),\n",
        "            wpe = nn.Embedding(block_size, n_embd),\n",
        "            drop = nn.Dropout(dropout),\n",
        "            h = nn.ModuleList([GPT2Block_sliding() for _ in range(n_layer)]),\n",
        "            ln_f = nn.LayerNorm((n_embd,), eps=1e-05, elementwise_affine=True),\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= block_size, f\"Cannot forward sequence of length {t}, block size is only {block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for GPT2Block_sliding in self.transformer.h:\n",
        "            x = GPT2Block_sliding(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "5KpVLwVNAYXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sliding=GPT_sliding()\n",
        "model_sliding_state_dict=model_sliding.state_dict()\n",
        "model_sliding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9hSTwtvArL9",
        "outputId": "d46eb357-6744-45a5-f962-869c0903718a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT_sliding(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(65, 384)\n",
              "    (wpe): Embedding(256, 384)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block_sliding(\n",
              "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention_sliding(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (pos): FixedPositionalEmbedding()\n",
              "          (sliding_window_attention): SlidingWindowAttention()\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model_sliding.parameters())/1e6, 'M parameters')\n",
        "#   print(name,torch.equal(model_query_state_dict[name],pretrained_state_dict[name]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScRPBbRyWNjG",
        "outputId": "e40d0534-8028-441b-9186-88556a78c470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.795776 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model_sliding.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "76hneeWvB1Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 100\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            X,Y=X.to(device),Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "FjPtvaMLB54X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sliding.eval()\n",
        "model_sliding=model_sliding.to(device)"
      ],
      "metadata": {
        "id": "YD4U918yWudM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(model_sliding)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    xb,yb=xb.to(device),yb.to(device)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model_sliding(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgHL12wBB78V",
        "outputId": "aa2d252c-e448-4add-f72a-05a8e315e61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3371, val loss 4.3359\n",
            "step 50: train loss 2.2397, val loss 2.2402\n",
            "step 100: train loss 1.9628, val loss 1.9595\n",
            "step 150: train loss 1.8090, val loss 1.8096\n",
            "step 200: train loss 1.7010, val loss 1.7063\n",
            "step 250: train loss 1.6387, val loss 1.6329\n",
            "step 300: train loss 1.5754, val loss 1.5727\n",
            "step 350: train loss 1.5313, val loss 1.5309\n",
            "step 400: train loss 1.4978, val loss 1.4969\n",
            "step 450: train loss 1.4628, val loss 1.4650\n",
            "step 500: train loss 1.4345, val loss 1.4396\n",
            "step 550: train loss 1.4158, val loss 1.4159\n",
            "step 600: train loss 1.3949, val loss 1.3950\n",
            "step 650: train loss 1.3686, val loss 1.3691\n",
            "step 700: train loss 1.3543, val loss 1.3538\n",
            "step 750: train loss 1.3416, val loss 1.3394\n",
            "step 800: train loss 1.3234, val loss 1.3265\n",
            "step 850: train loss 1.3100, val loss 1.3100\n",
            "step 900: train loss 1.3025, val loss 1.3035\n",
            "step 950: train loss 1.2930, val loss 1.2954\n",
            "step 999: train loss 1.2811, val loss 1.2810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model_sliding.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "-PpqFMlRXKq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748a87f1-e5db-4834-b13c-b2b64f25ff66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Thomager:\n",
            "An home, you should knows, made me and just.\n",
            "\n",
            "PETRUCHIO:\n",
            "Hie not plays fury your sallent filling woman\n",
            "Watch, and I sir, we sile you have so.\n",
            "\n",
            "CATESBY:\n",
            "My lord, thou letter. Hast he cousin, for Crince doth,\n",
            "Till on the drum to the trouble: it is it not give\n",
            "your heirs and a benedard\n",
            "To go fortable and with yet, Sainna, strainly pain\n",
            "Contented, and my wife, for the crowning behuted:\n",
            "A promish, singled at my leban speak all.\n",
            "Without Murder Juliet; let surmy.\n",
            "\n",
            "LUCIO:\n",
            "\n",
            "This down:\n",
            "I'll faul\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training function for training on sinle gpu.."
      ],
      "metadata": {
        "id": "3sjWeWStznCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "lCJUnLKyzmPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_id = torch.cuda.current_device()\n",
        "gpu_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6dnEGML7WTK",
        "outputId": "4ef42bb6-b96b-4c3a-8eb9-85e71a7cf647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: model,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int,\n",
        "    ) -> None:\n",
        "        self.gpu_id = gpu_id\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        logits, loss = model(source, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        # b_sz = len(next(iter(self.train_data))[0])\n",
        "        # print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "\n",
        "        source, targets = self.train_data\n",
        "        source = source.to(self.gpu_id)\n",
        "        targets = targets.to(self.gpu_id)\n",
        "        self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.state_dict()\n",
        "        PATH = \"checkpoint.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = get_batch('train') # load your dataset\n",
        "    model = GPT()  # load your model\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def main(device:int,total_epochs: int,save_every: int,batch_size:int):\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = dataset\n",
        "    trainer = Trainer(model, train_data, optimizer,gpu_id, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    total_epochs=100\n",
        "    save_every=50\n",
        "    device=gpu_id\n",
        "    batch_size = 32\n",
        "    main(device,total_epochs,save_every,batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXFeBRTz6ibw",
        "outputId": "8d2749d9-3b9d-4db9-ed61-4112a7711fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Training checkpoint saved at checkpoint.pt\n",
            "Epoch 50 | Training checkpoint saved at checkpoint.pt\n"
          ]
        }
      ]
    }
  ]
}